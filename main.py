"""
å®æ—¶è¯­éŸ³ç¿»è¯‘ç³»ç»Ÿ v4.0 - æ·±åº¦ä¼˜åŒ–ç‰ˆ

æ ¸å¿ƒæ”¹è¿›:
1. è‡ªé€‚åº” VADï¼šåŠ¨æ€è°ƒæ•´éŸ³é‡é˜ˆå€¼ï¼ˆé€‚åº”ä¸åŒç¯å¢ƒï¼‰
2. æ™ºèƒ½é™éŸ³æ£€æµ‹ï¼šåŸºäºèƒ½é‡è¡°å‡æ›²çº¿
3. å†…å­˜éŸ³é¢‘å¤„ç†ï¼šå‡å°‘æ–‡ä»¶ I/O
4. é‡è¯•æœºåˆ¶ï¼šç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•
5. æ’­æ”¾éš”ç¦»ï¼šæ’­æ”¾æœŸé—´ç¦ç”¨éº¦å…‹é£
6. å®Œå–„çš„è¿›åº¦æ˜¾ç¤º

ä¾èµ–: pip install faster-whisper requests pyaudio webrtcvad numpy
"""

import pyaudio
import wave
import io
import requests
import webrtcvad
import collections
import time
import numpy as np
from enum import Enum
from faster_whisper import WhisperModel
from typing import Optional
from dataclasses import dataclass

# ============== é…ç½® ==============
TRANSLATION_API_URL = "http://127.0.0.1:8099/v1/chat/completions"
TTS_API_URL = "http://127.0.0.1:9099/tts"

# éŸ³é¢‘å‚æ•°
SAMPLE_RATE = 16000
CHANNELS = 1
CHUNK_DURATION_MS = 30
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)

# VAD é…ç½®ï¼ˆä¼˜åŒ–åï¼‰
VAD_MODE = 2
SPEECH_START_FRAMES = 10       # è¿ç»­10å¸§æ‰è§¦å‘ï¼ˆ300msï¼‰
SPEECH_END_SILENCE_MS = 600    # é™éŸ³600msç»“æŸï¼ˆæ›´å¿«å“åº”ï¼‰
SPEECH_MIN_DURATION = 0.8      # æœ€çŸ­å½•éŸ³ï¼ˆç§’ï¼‰
SPEECH_MAX_DURATION = 20       # æœ€é•¿å½•éŸ³ï¼ˆç§’ï¼‰

# è‡ªé€‚åº”éŸ³é‡é˜ˆå€¼
VOLUME_THRESHOLD_INIT = 300    # åˆå§‹é˜ˆå€¼
VOLUME_THRESHOLD_MIN = 150     # æœ€å°é˜ˆå€¼
VOLUME_THRESHOLD_MAX = 800     # æœ€å¤§é˜ˆå€¼
CALIBRATION_FRAMES = 50        # æ ¡å‡†å¸§æ•°

# é‡è¯•é…ç½®
MAX_RETRIES = 2
TIMEOUT_TRANSLATION = 8
TIMEOUT_TTS = 15


class State(Enum):
    IDLE = "ç©ºé—²"
    CALIBRATING = "æ ¡å‡†ä¸­"
    LISTENING = "ç›‘å¬ä¸­"
    RECORDING = "å½•éŸ³ä¸­"
    PROCESSING = "å¤„ç†ä¸­"
    PLAYING = "æ’­æ”¾ä¸­"


@dataclass
class AudioStats:
    """éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯"""
    rms: float
    max_amplitude: int
    duration: float


class AdaptiveVAD:
    """è‡ªé€‚åº” VAD æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.vad = webrtcvad.Vad(VAD_MODE)
        self.volume_threshold = VOLUME_THRESHOLD_INIT
        self.noise_floor = 0
        self.calibrated = False
        
    def calibrate(self, stream, frames_count: int = CALIBRATION_FRAMES):
        """æ ¡å‡†ç¯å¢ƒå™ªéŸ³æ°´å¹³"""
        print(f"[æ ¡å‡†] æ­£åœ¨é‡‡æ ·ç¯å¢ƒå™ªéŸ³... (ä¿æŒå®‰é™ {frames_count * 30}ms)")
        
        noise_samples = []
        for i in range(frames_count):
            chunk = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            audio_np = np.frombuffer(chunk, dtype=np.int16)
            rms = np.sqrt(np.mean(audio_np.astype(float) ** 2))
            noise_samples.append(rms)
            
            # è¿›åº¦æ¡
            progress = (i + 1) / frames_count * 100
            bar_len = int(progress / 5)
            print(f"\r  è¿›åº¦: [{'â–ˆ' * bar_len}{'Â·' * (20 - bar_len)}] {progress:.0f}%", end='')
        
        print()
        
        # è®¡ç®—å™ªéŸ³åŸºçº¿ï¼ˆå–75åˆ†ä½æ•°ï¼Œæ’é™¤å¼‚å¸¸å€¼ï¼‰
        self.noise_floor = np.percentile(noise_samples, 75)
        self.volume_threshold = max(
            VOLUME_THRESHOLD_MIN,
            min(VOLUME_THRESHOLD_MAX, self.noise_floor * 2)
        )
        self.calibrated = True
        
        print(f"[æ ¡å‡†] âœ“ å®Œæˆ | å™ªéŸ³åŸºçº¿: {self.noise_floor:.0f} | é˜ˆå€¼: {self.volume_threshold:.0f}")
        
    def is_speech(self, chunk: bytes) -> tuple[bool, float]:
        """æ£€æµ‹æ˜¯å¦ä¸ºè¯­éŸ³ï¼Œè¿”å› (æ˜¯å¦è¯­éŸ³, RMSéŸ³é‡)"""
        audio_np = np.frombuffer(chunk, dtype=np.int16)
        rms = np.sqrt(np.mean(audio_np.astype(float) ** 2))
        
        # VAD æ£€æµ‹
        try:
            vad_result = self.vad.is_speech(chunk, SAMPLE_RATE)
        except:
            vad_result = False
        
        # éŸ³é‡æ£€æµ‹ï¼ˆç›¸å¯¹äºå™ªéŸ³åŸºçº¿ï¼‰
        volume_ok = rms > self.volume_threshold
        
        return vad_result and volume_ok, rms


class RealtimeTranslator:
    def __init__(self):
        self.vad = AdaptiveVAD()
        self.audio = pyaudio.PyAudio()
        self.whisper_model = None
        self.state = State.IDLE
        self.is_running = False
        self.stats = {'processed': 0, 'failed': 0, 'avg_time': 0}
        
    def log(self, msg: str, prefix: str = ""):
        """æ—¥å¿—è¾“å‡º"""
        timestamp = time.strftime('%H:%M:%S')
        print(f"[{timestamp}] {prefix}{msg}")
    
    def set_state(self, new_state: State):
        """åˆ‡æ¢çŠ¶æ€"""
        if self.state != new_state:
            self.state = new_state
            # ä¸æ‰“å°æ¯æ¬¡çŠ¶æ€åˆ‡æ¢ï¼Œé¿å…åˆ·å±
    
    def init_whisper(self):
        """åŠ è½½ Whisper æ¨¡å‹"""
        self.log("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ (base)...")
        self.whisper_model = WhisperModel(
            "base", 
            device="cpu", 
            compute_type="int8",
            num_workers=2
        )
        self.log("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ", "  ")
    
    def get_audio_stats(self, audio_data: bytes) -> AudioStats:
        """è®¡ç®—éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯"""
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        rms = np.sqrt(np.mean(audio_np.astype(float) ** 2))
        max_amp = np.max(np.abs(audio_np))
        duration = len(audio_np) / SAMPLE_RATE
        return AudioStats(rms=rms, max_amplitude=max_amp, duration=duration)
    
    def record_speech(self, stream) -> Optional[bytes]:
        """
        å½•éŸ³ç›´åˆ°æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
        ä½¿ç”¨æ»‘åŠ¨çª—å£ + èƒ½é‡è¡°å‡æ£€æµ‹
        """
        self.set_state(State.LISTENING)
        
        # ç­‰å¾…è¯­éŸ³å¼€å§‹
        start_buffer = collections.deque(maxlen=SPEECH_START_FRAMES)
        
        self.log("ğŸ§ ç­‰å¾…è¯´è¯...")
        while self.is_running:
            chunk = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            is_voice, rms = self.vad.is_speech(chunk)
            start_buffer.append((chunk, is_voice, rms))
            
            # è®¡ç®—è¯­éŸ³å¸§æ¯”ä¾‹
            voice_ratio = sum(1 for _, v, _ in start_buffer if v) / len(start_buffer)
            
            # 85% ä»¥ä¸Šçš„å¸§æ˜¯è¯­éŸ³æ‰è§¦å‘ï¼ˆæ¯”ä¹‹å‰æ›´ä¸¥æ ¼ï¼‰
            if voice_ratio >= 0.85 and len(start_buffer) == SPEECH_START_FRAMES:
                break
        
        if not self.is_running:
            return None
        
        # å¼€å§‹å½•éŸ³
        self.set_state(State.RECORDING)
        self.log("ğŸ¤ å½•éŸ³ä¸­...", "  ")
        
        frames = [f for f, _, _ in start_buffer]
        silence_frames = 0
        record_start = time.time()
        max_rms = max(r for _, _, r in start_buffer)  # è®°å½•å³°å€¼éŸ³é‡
        
        # å®æ—¶è¿›åº¦æ˜¾ç¤ºï¼ˆå•è¡Œæ›´æ–°ï¼‰
        last_update = time.time()
        
        while self.is_running:
            chunk = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            frames.append(chunk)
            
            is_voice, rms = self.vad.is_speech(chunk)
            duration = time.time() - record_start
            
            # æ›´æ–°å³°å€¼éŸ³é‡
            if rms > max_rms:
                max_rms = rms
            
            # è¿›åº¦æ˜¾ç¤ºï¼ˆ100ms æ›´æ–°ä¸€æ¬¡ï¼‰
            if time.time() - last_update > 0.1:
                indicator = "ğŸ”Š" if is_voice else "  "
                bar_len = min(int(duration * 2), 30)  # æœ€å¤š30ä¸ªå­—ç¬¦
                bar = "â–ˆ" * bar_len
                print(f"\r  âºï¸  {duration:4.1f}s {indicator} [{bar:<30}] RMS:{rms:4.0f}", end='', flush=True)
                last_update = time.time()
            
            # é™éŸ³æ£€æµ‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰
            if is_voice:
                silence_frames = 0
            else:
                silence_frames += 1
                silence_ms = silence_frames * CHUNK_DURATION_MS
                
                # é™éŸ³è¾¾åˆ°é˜ˆå€¼ + éŸ³é‡æ˜æ˜¾ä¸‹é™
                if silence_ms >= SPEECH_END_SILENCE_MS:
                    # æ£€æŸ¥éŸ³é‡æ˜¯å¦è¡°å‡åˆ°å³°å€¼çš„30%ä»¥ä¸‹
                    if rms < max_rms * 0.3:
                        print()  # æ¢è¡Œ
                        self.log(f"âœ“ å½•éŸ³ç»“æŸ (æ—¶é•¿: {duration:.1f}s)", "  ")
                        break
            
            # è¶…æ—¶ä¿æŠ¤
            if duration >= SPEECH_MAX_DURATION:
                print()
                self.log(f"â±ï¸  è¾¾åˆ°æœ€å¤§æ—¶é•¿ ({SPEECH_MAX_DURATION}s)", "  ")
                break
        
        if not self.is_running:
            return None
        
        # æ£€æŸ¥å½•éŸ³è´¨é‡
        audio_data = b''.join(frames)
        stats = self.get_audio_stats(audio_data)
        
        if stats.duration < SPEECH_MIN_DURATION:
            self.log(f"âš ï¸  å½•éŸ³å¤ªçŸ­ ({stats.duration:.1f}s < {SPEECH_MIN_DURATION}s)ï¼Œå·²å¿½ç•¥", "  ")
            return None
        
        if stats.max_amplitude < 500:
            self.log(f"âš ï¸  éŸ³é‡è¿‡ä½ (å³°å€¼: {stats.max_amplitude})ï¼Œå·²å¿½ç•¥", "  ")
            return None
        
        return audio_data
    
    def asr_transcribe(self, audio_data: bytes) -> Optional[str]:
        """ASR: å†…å­˜éŸ³é¢‘ â†’ æ–‡æœ¬ï¼ˆé¿å…æ–‡ä»¶ I/Oï¼‰"""
        # åˆ›å»ºå†…å­˜ WAV æ–‡ä»¶
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(audio_data)
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆFaster-Whisper æš‚ä¸æ”¯æŒ BytesIOï¼‰
        temp_file = "/tmp/realtime_audio.wav"
        with open(temp_file, 'wb') as f:
            f.write(wav_buffer.getvalue())
        
        try:
            segments, info = self.whisper_model.transcribe(
                temp_file,
                beam_size=5,
                language="zh",
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=300)
            )
            text = "".join(seg.text for seg in segments).strip()
            return text if text else None
        except Exception as e:
            self.log(f"âŒ ASR é”™è¯¯: {e}", "  ")
            return None
    
    def translate_with_retry(self, text: str, retries: int = MAX_RETRIES) -> Optional[str]:
        """ç¿»è¯‘ï¼ˆå¸¦é‡è¯•ï¼‰"""
        for attempt in range(retries):
            try:
                response = requests.post(
                    TRANSLATION_API_URL,
                    json={
                        "model": "HY-MT1.5-1.8B",
                        "messages": [{
                            "role": "user",
                            "content": f"Translate the following segment into English, without additional explanation.\n\n{text}"
                        }],
                        "stream": False
                    },
                    headers={"Authorization": "Bearer sk-1234", "Content-Type": "application/json"},
                    timeout=TIMEOUT_TRANSLATION
                )
                response.raise_for_status()
                result = response.json()["choices"][0]["message"]["content"].strip()
                return result if result else None
            except requests.Timeout:
                if attempt < retries - 1:
                    self.log(f"âš ï¸  ç¿»è¯‘è¶…æ—¶ï¼Œé‡è¯• {attempt + 1}/{retries}...", "  ")
                    time.sleep(0.5)
                else:
                    self.log(f"âŒ ç¿»è¯‘å¤±è´¥: è¶…æ—¶", "  ")
            except Exception as e:
                if attempt < retries - 1:
                    self.log(f"âš ï¸  ç¿»è¯‘é”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{retries}...", "  ")
                    time.sleep(0.5)
                else:
                    self.log(f"âŒ ç¿»è¯‘å¤±è´¥: {e}", "  ")
        return None
    
    def process_audio(self, audio_data: bytes) -> Optional[str]:
        """å¤„ç†éŸ³é¢‘ï¼šASR + ç¿»è¯‘"""
        self.set_state(State.PROCESSING)
        
        # ASR
        self.log("ğŸ“ è¯†åˆ«ä¸­...", "  ")
        chinese_text = self.asr_transcribe(audio_data)
        if not chinese_text:
            self.log("âš ï¸  æœªè¯†åˆ«åˆ°æ–‡æœ¬", "  ")
            return None
        self.log(f"   ä¸­æ–‡: {chinese_text}", "  ")
        
        # ç¿»è¯‘
        self.log("ğŸŒ ç¿»è¯‘ä¸­...", "  ")
        english_text = self.translate_with_retry(chinese_text)
        if not english_text:
            return None
        self.log(f"   è‹±æ–‡: {english_text}", "  ")
        
        return english_text
    
    def play_tts(self, text: str, stream):
        """TTS å¹¶æ’­æ”¾ï¼ˆæ’­æ”¾æœŸé—´æš‚åœå½•éŸ³ï¼‰"""
        self.set_state(State.PLAYING)
        self.log("ğŸ”Š ç”Ÿæˆè¯­éŸ³...", "  ")
        
        # æš‚åœéº¦å…‹é£
        stream.stop_stream()
        
        try:
            # TTS
            response = requests.post(
                TTS_API_URL,
                data={"text": text},
                stream=True,
                timeout=TIMEOUT_TTS
            )
            response.raise_for_status()
            
            # ä¿å­˜éŸ³é¢‘
            temp_file = "/tmp/tts_output.wav"
            with open(temp_file, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # æ’­æ”¾
            wf = wave.open(temp_file, 'rb')
            play_stream = self.audio.open(
                format=self.audio.get_format_from_width(wf.getsampwidth()),
                channels=wf.getnchannels(),
                rate=wf.getframerate(),
                output=True
            )
            
            self.log("ğŸ”Š æ’­æ”¾ä¸­...", "  ")
            data = wf.readframes(1024)
            while data and self.is_running:
                play_stream.write(data)
                data = wf.readframes(1024)
            
            play_stream.stop_stream()
            play_stream.close()
            wf.close()
            
            self.log("âœ“ æ’­æ”¾å®Œæˆ", "  ")
            
        except Exception as e:
            self.log(f"âŒ TTS/æ’­æ”¾é”™è¯¯: {e}", "  ")
        finally:
            # æ¢å¤éº¦å…‹é£ï¼ˆå»¶è¿Ÿ500msï¼Œç¡®ä¿å›å£°æ¶ˆæ•£ï¼‰
            time.sleep(0.5)
            stream.start_stream()
    
    def run_cycle(self, stream) -> bool:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´å¾ªç¯"""
        start_time = time.time()
        
        # 1. å½•éŸ³
        audio_data = self.record_speech(stream)
        if audio_data is None:
            return self.is_running
        
        # 2. å¤„ç†
        english_text = self.process_audio(audio_data)
        if english_text is None:
            self.stats['failed'] += 1
            return self.is_running
        
        # 3. æ’­æ”¾
        self.play_tts(english_text, stream)
        
        # ç»Ÿè®¡
        elapsed = time.time() - start_time
        self.stats['processed'] += 1
        self.stats['avg_time'] = (
            (self.stats['avg_time'] * (self.stats['processed'] - 1) + elapsed)
            / self.stats['processed']
        )
        
        self.log(f"âœ“ æœ¬è½®å®Œæˆ (è€—æ—¶: {elapsed:.1f}s)", "  ")
        print()  # ç©ºè¡Œåˆ†éš”
        
        return self.is_running
    
    def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        self.is_running = True
        
        # æ‰“å°æ¬¢è¿
        print(f"\n{'='*60}")
        print("ğŸŒ å®æ—¶è¯­éŸ³ç¿»è¯‘ç³»ç»Ÿ v4.0")
        print(f"{'='*60}\n")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.init_whisper()
        
        # æ‰“å¼€éº¦å…‹é£
        try:
            stream = self.audio.open(
                format=pyaudio.paInt16,
                channels=CHANNELS,
                rate=SAMPLE_RATE,
                input=True,
                frames_per_buffer=CHUNK_SIZE
            )
        except Exception as e:
            self.log(f"âŒ æ— æ³•æ‰“å¼€éº¦å…‹é£: {e}")
            return
        
        # æ ¡å‡†ç¯å¢ƒå™ªéŸ³
        self.vad.calibrate(stream)
        
        print(f"\n{'='*60}")
        print("ğŸ“‹ ç³»ç»Ÿå°±ç»ª")
        print(f"   æµç¨‹: ç›‘å¬ â†’ å½•éŸ³ â†’ è¯†åˆ« â†’ ç¿»è¯‘ â†’ æ’­æ”¾")
        print(f"   é…ç½®: é™éŸ³ {SPEECH_END_SILENCE_MS}ms ç»“æŸ | æ—¶é•¿ {SPEECH_MIN_DURATION}-{SPEECH_MAX_DURATION}s")
        print(f"   æ“ä½œ: æ­£å¸¸è¯´è¯å³å¯ | Ctrl+C é€€å‡º")
        print(f"{'='*60}\n")
        
        try:
            while self.is_running:
                if not self.run_cycle(stream):
                    break
        except KeyboardInterrupt:
            print("\n")
            self.log("ç”¨æˆ·ä¸­æ–­")
        finally:
            self.is_running = False
            stream.stop_stream()
            stream.close()
            self.audio.terminate()
            
            # æ‰“å°ç»Ÿè®¡
            print(f"\n{'='*60}")
            print("ğŸ“Š è¿è¡Œç»Ÿè®¡")
            print(f"   æˆåŠŸ: {self.stats['processed']} | å¤±è´¥: {self.stats['failed']}")
            if self.stats['processed'] > 0:
                print(f"   å¹³å‡è€—æ—¶: {self.stats['avg_time']:.1f}s")
            print(f"{'='*60}")
            self.log("ç³»ç»Ÿå·²é€€å‡º ğŸ‘‹")


if __name__ == "__main__":
    translator = RealtimeTranslator()
    translator.start()