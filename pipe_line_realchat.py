"""
å®æ—¶è‹±è¯­å¯¹è¯åŠ©æ‰‹ v1.0

æµç¨‹:
1. è¯­éŸ³å½•åˆ¶ (VAD è‡ªé€‚åº”æ£€æµ‹)
2. ASR è¯†åˆ« (Whisper - ä¸­æ–‡â†’æ–‡æœ¬)
3. LLM å¯¹è¯ (OpenAI å…¼å®¹ API - è‹±è¯­ä¼™ä¼´åŠ©æ‰‹)
4. TTS æ’­æ”¾ (Kyutai TTS - æ–‡æœ¬â†’è¯­éŸ³)

ç‰¹æ€§:
- æ— çŠ¶æ€å¯¹è¯ (æ¯è½®ç‹¬ç«‹ï¼Œä¸æºå¸¦å†å²)
- è‹±è¯­å­¦ä¹ ä¼™ä¼´é£æ ¼
- è‡ªé€‚åº”å™ªéŸ³æ ¡å‡†
- æ’­æ”¾æœŸé—´ç¦ç”¨éº¦å…‹é£ (é¿å…å›å£°)

ä¾èµ–: pip install faster-whisper requests pyaudio webrtcvad numpy openai
"""

import pyaudio
import wave
import io
import requests
import webrtcvad
import collections
import time
import json
import numpy as np
import threading
from enum import Enum
from faster_whisper import WhisperModel
from typing import Optional
from dataclasses import dataclass

# ============== é…ç½® ==============
LLM_API_URL = "http://127.0.0.1:6001/v1/chat/completions"
TTS_API_URL = "http://127.0.0.1:9099/tts"

# éŸ³é¢‘å‚æ•°
SAMPLE_RATE = 16000
CHANNELS = 1
CHUNK_DURATION_MS = 30
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)

# VAD é…ç½®
VAD_MODE = 2
SPEECH_START_FRAMES = 10
SPEECH_END_SILENCE_MS = 600
SPEECH_MIN_DURATION = 0.8
SPEECH_MAX_DURATION = 20

# è‡ªé€‚åº”éŸ³é‡é˜ˆå€¼
VOLUME_THRESHOLD_INIT = 300
VOLUME_THRESHOLD_MIN = 150
VOLUME_THRESHOLD_MAX = 800
CALIBRATION_FRAMES = 50

# é‡è¯•é…ç½®
MAX_RETRIES = 2
TIMEOUT_LLM = 8
TIMEOUT_TTS = 15

# LLM é…ç½®
LLM_MODEL = "Qwen3-0.6B"
SYSTEM_PROMPT = (
    "You are a friendly English conversation partner helper. "
    "The user is learning English and will speak in Chinese. "
    "Respond naturally in English to help them practice. "
    "Keep responses concise (1-2 sentences) and conversational. "
    "Be encouraging and helpful."
)


class State(Enum):
    IDLE = "ç©ºé—²"
    CALIBRATING = "æ ¡å‡†ä¸­"
    LISTENING = "ç›‘å¬ä¸­"
    RECORDING = "å½•éŸ³ä¸­"
    PROCESSING = "å¤„ç†ä¸­"
    PLAYING = "æ’­æ”¾ä¸­"


@dataclass
class AudioStats:
    """éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯"""
    rms: float
    max_amplitude: int
    duration: float


class AdaptiveVAD:
    """è‡ªé€‚åº” VAD æ£€æµ‹å™¨"""

    def __init__(self):
        self.vad = webrtcvad.Vad(VAD_MODE)
        self.volume_threshold = VOLUME_THRESHOLD_INIT
        self.noise_floor = 0
        self.calibrated = False

    def calibrate(self, stream, frames_count: int = CALIBRATION_FRAMES):
        """æ ¡å‡†ç¯å¢ƒå™ªéŸ³æ°´å¹³"""
        print(f"[æ ¡å‡†] æ­£åœ¨é‡‡æ ·ç¯å¢ƒå™ªéŸ³... (ä¿æŒå®‰é™ {frames_count * 30}ms)")

        noise_samples = []
        for i in range(frames_count):
            chunk = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            audio_np = np.frombuffer(chunk, dtype=np.int16)
            rms = np.sqrt(np.mean(audio_np.astype(float) ** 2))
            noise_samples.append(rms)

            progress = (i + 1) / frames_count * 100
            bar_len = int(progress / 5)
            print(f"\r  è¿›åº¦: [{'â–ˆ' * bar_len}{'Â·' * (20 - bar_len)}] {progress:.0f}%", end='')

        print()

        self.noise_floor = np.percentile(noise_samples, 75)
        self.volume_threshold = max(
            VOLUME_THRESHOLD_MIN,
            min(VOLUME_THRESHOLD_MAX, self.noise_floor * 2)
        )
        self.calibrated = True

        print(f"[æ ¡å‡†] âœ“ å®Œæˆ | å™ªéŸ³åŸºçº¿: {self.noise_floor:.0f} | é˜ˆå€¼: {self.volume_threshold:.0f}")

    def is_speech(self, chunk: bytes) -> tuple[bool, float]:
        """æ£€æµ‹æ˜¯å¦ä¸ºè¯­éŸ³ï¼Œè¿”å› (æ˜¯å¦è¯­éŸ³, RMSéŸ³é‡)"""
        audio_np = np.frombuffer(chunk, dtype=np.int16)
        rms = np.sqrt(np.mean(audio_np.astype(float) ** 2))

        try:
            vad_result = self.vad.is_speech(chunk, SAMPLE_RATE)
        except:
            vad_result = False

        volume_ok = rms > self.volume_threshold

        return vad_result and volume_ok, rms


class EnglishChatPartner:
    """è‹±è¯­å¯¹è¯åŠ©æ‰‹"""

    def __init__(self):
        self.vad = AdaptiveVAD()
        self.audio = pyaudio.PyAudio()
        self.whisper_model = None
        self.state = State.IDLE
        self.is_running = False
        self.stats = {'processed': 0, 'failed': 0, 'avg_time': 0}

    def log(self, msg: str, prefix: str = ""):
        """æ—¥å¿—è¾“å‡º"""
        timestamp = time.strftime('%H:%M:%S')
        print(f"[{timestamp}] {prefix}{msg}")

    def set_state(self, new_state: State):
        """åˆ‡æ¢çŠ¶æ€"""
        if self.state != new_state:
            self.state = new_state

    def _bytes_to_wav_bytes(self, audio_data: bytes) -> bytes:
        """Convert raw audio bytes to WAV format in memory"""
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(audio_data)
        return wav_buffer.getvalue()

    def _load_whisper_in_background(self):
        """åå°åŠ è½½ Whisper æ¨¡å‹"""
        self.whisper_model = WhisperModel(
            "base",
            device="cpu",
            compute_type="int8",
            num_workers=2
        )

    def init_whisper(self, wait=True):
        """åŠ è½½ Whisper æ¨¡å‹"""
        if wait:
            self.log("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ (base)...")
            self._load_whisper_in_background()
            self.log("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ", "  ")
        else:
            # åå°åŠ è½½ï¼Œä¸ç­‰å¾…
            thread = threading.Thread(target=self._load_whisper_in_background, daemon=True)
            thread.start()
            return thread

    def get_audio_stats(self, audio_data: bytes) -> AudioStats:
        """è®¡ç®—éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯"""
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        rms = np.sqrt(np.mean(audio_np.astype(float) ** 2))
        max_amp = np.max(np.abs(audio_np))
        duration = len(audio_data) / SAMPLE_RATE
        return AudioStats(rms=rms, max_amplitude=max_amp, duration=duration)

    def record_speech(self, stream) -> Optional[bytes]:
        """å½•éŸ³ç›´åˆ°æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ"""
        self.set_state(State.LISTENING)

        # ç­‰å¾…è¯­éŸ³å¼€å§‹
        start_buffer = collections.deque(maxlen=SPEECH_START_FRAMES)

        self.log("ğŸ§ ç­‰å¾…è¯´è¯...")
        while self.is_running:
            chunk = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            is_voice, rms = self.vad.is_speech(chunk)
            start_buffer.append((chunk, is_voice, rms))

            voice_ratio = sum(1 for _, v, _ in start_buffer if v) / len(start_buffer)

            if voice_ratio >= 0.85 and len(start_buffer) == SPEECH_START_FRAMES:
                break

        if not self.is_running:
            return None

        # å¼€å§‹å½•éŸ³
        self.set_state(State.RECORDING)
        self.log("ğŸ¤ å½•éŸ³ä¸­...", "  ")

        frames = [f for f, _, _ in start_buffer]
        silence_frames = 0
        record_start = time.time()
        max_rms = max(r for _, _, r in start_buffer)

        last_update = time.time()

        while self.is_running:
            chunk = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            frames.append(chunk)

            is_voice, rms = self.vad.is_speech(chunk)
            duration = time.time() - record_start

            if rms > max_rms:
                max_rms = rms

            if time.time() - last_update > 0.1:
                indicator = "ğŸ”Š" if is_voice else "  "
                bar_len = min(int(duration * 2), 30)
                bar = "â–ˆ" * bar_len
                print(f"\r  âºï¸  {duration:4.1f}s {indicator} [{bar:<30}] RMS:{rms:4.0f}", end='', flush=True)
                last_update = time.time()

            # é™éŸ³æ£€æµ‹
            if is_voice:
                silence_frames = 0
            else:
                silence_frames += 1
                silence_ms = silence_frames * CHUNK_DURATION_MS

                if silence_ms >= SPEECH_END_SILENCE_MS:
                    if rms < max_rms * 0.3:
                        print()
                        self.log(f"âœ“ å½•éŸ³ç»“æŸ (æ—¶é•¿: {duration:.1f}s)", "  ")
                        break

            # è¶…æ—¶ä¿æŠ¤
            if duration >= SPEECH_MAX_DURATION:
                print()
                self.log(f"â±ï¸  è¾¾åˆ°æœ€å¤§æ—¶é•¿ ({SPEECH_MAX_DURATION}s)", "  ")
                break

        if not self.is_running:
            return None

        # æ£€æŸ¥å½•éŸ³è´¨é‡
        audio_data = b''.join(frames)
        stats = self.get_audio_stats(audio_data)

        if stats.duration < SPEECH_MIN_DURATION:
            self.log(f"âš ï¸  å½•éŸ³å¤ªçŸ­ ({stats.duration:.1f}s < {SPEECH_MIN_DURATION}s)ï¼Œå·²å¿½ç•¥", "  ")
            return None

        if stats.max_amplitude < 500:
            self.log(f"âš ï¸  éŸ³é‡è¿‡ä½ (å³°å€¼: {stats.max_amplitude})ï¼Œå·²å¿½ç•¥", "  ")
            return None

        return audio_data

    def asr_transcribe(self, audio_data: bytes) -> Optional[str]:
        """ASR: éŸ³é¢‘ â†’ æ–‡æœ¬"""
        wav_bytes = self._bytes_to_wav_bytes(audio_data)
        wav_io = io.BytesIO(wav_bytes)

        try:
            segments, info = self.whisper_model.transcribe(
                wav_io,
                beam_size=5,
                language="zh"
            )
            text = "".join(seg.text for seg in segments).strip()
            return text if text else None
        except Exception as e:
            self.log(f"âŒ ASR é”™è¯¯: {e}", "  ")
            return None

    def chat_with_llm(self, text: str, retries: int = MAX_RETRIES) -> Optional[str]:
        """LLM å¯¹è¯ (æ— çŠ¶æ€)"""
        for attempt in range(retries):
            try:
                response = requests.post(
                    LLM_API_URL,
                    json={
                        "model": LLM_MODEL,
                        "messages": [
                            {"role": "system", "content": SYSTEM_PROMPT},
                            {"role": "user", "content": text}
                        ],
                        "max_tokens": 256,
                        "stream": False
                    },
                    headers={"Content-Type": "application/json"},
                    timeout=TIMEOUT_LLM
                )
                response.raise_for_status()
                result = response.json()["choices"][0]["message"]["content"].strip()
                return result if result else None
            except requests.Timeout:
                if attempt < retries - 1:
                    self.log(f"âš ï¸  LLM è¶…æ—¶ï¼Œé‡è¯• {attempt + 1}/{retries}...", "  ")
                    time.sleep(0.5)
                else:
                    self.log(f"âŒ LLM å¤±è´¥: è¶…æ—¶", "  ")
            except Exception as e:
                if attempt < retries - 1:
                    self.log(f"âš ï¸  LLM é”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{retries}...", "  ")
                    time.sleep(0.5)
                else:
                    self.log(f"âŒ LLM å¤±è´¥: {e}", "  ")
        return None

    def chat_with_llm_stream(self, text: str, retries: int = MAX_RETRIES) -> Optional[str]:
        """LLM å¯¹è¯ (æµå¼ï¼Œå¯æå‰å¼€å§‹TTS)"""
        for attempt in range(retries):
            try:
                response = requests.post(
                    LLM_API_URL,
                    json={
                        "model": LLM_MODEL,
                        "messages": [
                            {"role": "system", "content": SYSTEM_PROMPT},
                            {"role": "user", "content": text}
                        ],
                        "max_tokens": 256,
                        "stream": True
                    },
                    headers={"Content-Type": "application/json"},
                    timeout=TIMEOUT_LLM,
                    stream=True
                )
                response.raise_for_status()

                full_text = ""
                for line in response.iter_lines():
                    if not line:
                        continue
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            chunk = json.loads(data)
                            if 'choices' in chunk and chunk['choices']:
                                delta = chunk['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                if content:
                                    full_text += content
                        except:
                            pass

                return full_text.strip() if full_text else None

            except requests.Timeout:
                if attempt < retries - 1:
                    self.log(f"âš ï¸  LLM è¶…æ—¶ï¼Œé‡è¯• {attempt + 1}/{retries}...", "  ")
                    time.sleep(0.5)
                else:
                    self.log(f"âŒ LLM å¤±è´¥: è¶…æ—¶", "  ")
            except Exception as e:
                if attempt < retries - 1:
                    self.log(f"âš ï¸  LLM é”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{retries}...", "  ")
                    time.sleep(0.5)
                else:
                    self.log(f"âŒ LLM å¤±è´¥: {e}", "  ")
        return None

    def process_audio(self, audio_data: bytes) -> Optional[str]:
        """å¤„ç†éŸ³é¢‘ï¼šASR + LLM"""
        self.set_state(State.PROCESSING)

        # ASR
        self.log("ğŸ“ è¯†åˆ«ä¸­...", "  ")
        user_text = self.asr_transcribe(audio_data)
        if not user_text:
            self.log("âš ï¸  æœªè¯†åˆ«åˆ°æ–‡æœ¬", "  ")
            return None
        self.log(f"   ç”¨æˆ·: {user_text}", "  ")

        # LLM (ä½¿ç”¨æµå¼ï¼Œé™ä½é¦–å­—å»¶è¿Ÿ)
        self.log("ğŸ¤– æ€è€ƒä¸­...", "  ")
        response = self.chat_with_llm_stream(user_text)
        if not response:
            return None
        self.log(f"   åŠ©æ‰‹: {response}", "  ")

        return response

    def play_tts(self, text: str, stream):
        """TTS å¹¶æ’­æ”¾ (å†…å­˜ç¼“å†²ï¼Œæ— ç£ç›˜IO)"""
        self.set_state(State.PLAYING)
        self.log("ğŸ”Š ç”Ÿæˆè¯­éŸ³...", "  ")

        stream.stop_stream()

        try:
            response = requests.post(
                TTS_API_URL,
                data={"text": text},
                timeout=TIMEOUT_TTS
            )
            response.raise_for_status()

            # ç›´æ¥åœ¨å†…å­˜ä¸­å¤„ç† WAV
            wav_io = io.BytesIO(response.content)

            with wave.open(wav_io, 'rb') as wf:
                play_stream = self.audio.open(
                    format=self.audio.get_format_from_width(wf.getsampwidth()),
                    channels=wf.getnchannels(),
                    rate=wf.getframerate(),
                    output=True
                )

            self.log("ğŸ”Š æ’­æ”¾ä¸­...", "  ")

            # ä»å†…å­˜æµå¼æ’­æ”¾
            wav_io.seek(44)  # è·³è¿‡ WAV header
            while self.is_running:
                data = wav_io.read(1024)
                if not data:
                    break
                play_stream.write(data)

            play_stream.stop_stream()
            play_stream.close()

            self.log("âœ“ æ’­æ”¾å®Œæˆ", "  ")

        except Exception as e:
            self.log(f"âŒ TTS/æ’­æ”¾é”™è¯¯: {e}", "  ")
        finally:
            time.sleep(0.5)
            stream.start_stream()

    def run_cycle(self, stream) -> bool:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´å¾ªç¯"""
        start_time = time.time()

        # 1. å½•éŸ³
        audio_data = self.record_speech(stream)
        if audio_data is None:
            return self.is_running

        # 2. å¤„ç†
        response_text = self.process_audio(audio_data)
        if response_text is None:
            self.stats['failed'] += 1
            return self.is_running

        # 3. æ’­æ”¾
        self.play_tts(response_text, stream)

        # ç»Ÿè®¡
        elapsed = time.time() - start_time
        self.stats['processed'] += 1
        self.stats['avg_time'] = (
            (self.stats['avg_time'] * (self.stats['processed'] - 1) + elapsed)
            / self.stats['processed']
        )

        self.log(f"âœ“ æœ¬è½®å®Œæˆ (è€—æ—¶: {elapsed:.1f}s)", "  ")
        print()

        return self.is_running

    def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        self.is_running = True

        print(f"\n{'='*60}")
        print("ğŸ—£ï¸  å®æ—¶è‹±è¯­å¯¹è¯åŠ©æ‰‹ v1.0 (ä¼˜åŒ–ç‰ˆ)")
        print(f"{'='*60}\n")

        # å¹¶è¡ŒåŠ è½½: å¯åŠ¨æ¨¡å‹åŠ è½½çº¿ç¨‹
        print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–...")
        model_thread = self.init_whisper(wait=False)

        try:
            stream = self.audio.open(
                format=pyaudio.paInt16,
                channels=CHANNELS,
                rate=SAMPLE_RATE,
                input=True,
                frames_per_buffer=CHUNK_SIZE
            )
        except Exception as e:
            self.log(f"âŒ æ— æ³•æ‰“å¼€éº¦å…‹é£: {e}")
            return

        # æ ¡å‡†æœŸé—´æ¨¡å‹åœ¨åå°åŠ è½½
        self.vad.calibrate(stream)

        # ç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆ
        if model_thread:
            self.log("â³ ç­‰å¾…æ¨¡å‹åŠ è½½...", "  ")
            model_thread.join()
            self.log("âœ“ æ¨¡å‹å°±ç»ª", "  ")

        print(f"\n{'='*60}")
        print("ğŸ“‹ ç³»ç»Ÿå°±ç»ª")
        print(f"   æµç¨‹: ç›‘å¬ â†’ å½•éŸ³ â†’ è¯†åˆ« â†’ å¯¹è¯ â†’ æ’­æ”¾")
        print(f"   æ¨¡å¼: æ— çŠ¶æ€å¯¹è¯ (æ¯è½®ç‹¬ç«‹)")
        print(f"   æ“ä½œ: è¯´ä¸­æ–‡ç»ƒä¹ è‹±è¯­ | Ctrl+C é€€å‡º")
        print(f"{'='*60}\n")

        try:
            while self.is_running:
                if not self.run_cycle(stream):
                    break
        except KeyboardInterrupt:
            print("\n")
            self.log("ç”¨æˆ·ä¸­æ–­")
        finally:
            self.is_running = False
            stream.stop_stream()
            stream.close()
            self.audio.terminate()

            print(f"\n{'='*60}")
            print("ğŸ“Š è¿è¡Œç»Ÿè®¡")
            print(f"   æˆåŠŸ: {self.stats['processed']} | å¤±è´¥: {self.stats['failed']}")
            if self.stats['processed'] > 0:
                print(f"   å¹³å‡è€—æ—¶: {self.stats['avg_time']:.1f}s")
            print(f"{'='*60}")
            self.log("ç³»ç»Ÿå·²é€€å‡º ğŸ‘‹")


if __name__ == "__main__":
    partner = EnglishChatPartner()
    partner.start()
